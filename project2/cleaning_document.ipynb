{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set (Training and Test set)\n",
    "\n",
    "We used IMDb Dataset which can be downloaded from [here](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "This data set contains 50,000 reviews which is evenly split into two groups: 25,000 reviews for each of training and testing. The reviews for training and testing data sets contain a disjoint set of movies. Therefore, we can assume that the validation result with testing data set can be applicable for other movie reviews.\n",
    "\n",
    "Each group has the same number of positive and negative reviews: a positive review has a score from 7 to 10 while a negative review has a score from 1 to 4. The reviews having score 5 or 6 are excluded to avoid vagueness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "For this project, we used my own Linux machine having AMD Ryzen 7 2700X, 16GB Memory, Geforce RTX 2070.\n",
    "In addition, Keras with Tensorflow backend is used for making a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import os \n",
    "#from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "#\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob\n",
    "#from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "from keras_tqdm import TQDMNotebookCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow initial setup\n",
    "\n",
    "To allow Tensorflow to use enough GPU memory, *allow_growth* option is turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data set files\n",
    "\n",
    "First of all, all the documents are loaded. The data sets for training and testing are stored in *data/train* and *data/test*, respectively. For each data set, positive and negative reviews are stored in *pos* and *neg* sub-directories.\n",
    "\n",
    "I have attached the progress bars using the [tqdm](https://github.com/tqdm/tqdm), which is useful in dealting with large data by allowing us to estimate each time of the stages.\n",
    "\n",
    "Referenced article for tqdm: https://towardsdatascience.com/progress-bars-in-python-4b44e8a4c482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3f4863e3af471ca99947d40866f626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667f782d54474880953099e5249103ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851cc7a47709405f977d5d17eac54e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da40b967fc2445a38c39bf07524e3fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load all docs in a directory\n",
    "def load_docs(directory):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        with open(path, 'r') as f:\n",
    "            # load the doc\n",
    "            doc = f.read()\n",
    "            # add to list\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# load all training reviews\n",
    "print(\"Loading training-positive-docs\")\n",
    "global_train_positive_docs = load_docs('data/train/pos')\n",
    "print(\"Loading training-negative-docs\")\n",
    "global_train_negative_docs = load_docs('data/train/neg')\n",
    "# load all test reviews\n",
    "print(\"Loading test-positive-docs\")\n",
    "global_test_positive_docs = load_docs('data/test/pos')\n",
    "print(\"Loading test-negative-docs\")\n",
    "global_test_negative_docs = load_docs('data/test/neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning documents\n",
    "\n",
    "### Pre-processing techniques\n",
    "\n",
    "In most of NLP releated works, documents are normally pre-processed to get better performance.\n",
    "We tried to apply several techniques which are well-known as follows:\n",
    "\n",
    "**1. Removing punctuations**  \n",
    "Normally punctuations do not have any meaning, but they exist for understandability. Therefore, such punctuations should be removed. But, we did not remove the apostrophe mark (') since such removing caused the incorrect stemming.\n",
    "\n",
    "**2. Removing stopwords**  \n",
    "We filtered out the stopwords.\n",
    "The stop words are those words that do not contribute to the deeper meaning of the phrase.\n",
    "They are the most common words such as: “the“, “a“, and “is“.\n",
    "NLTK provides a list of commonly agreed upon stop words for a variety of languages.\n",
    "\n",
    "**3. Stemming**    \n",
    "The *PorterStemmer* is provided in *NLTK python package*.\n",
    "We made the words into lowercases and used the stemming method in order to both reduce the vocabulary and to focus on the sense or sentiment of a document rather than deeper meaning.\n",
    "\n",
    "**4. Removing non-frequent words**   \n",
    "It is important to define a vocabulary of known words when using a bag-of-words or embedding model.\n",
    "The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. \n",
    "\n",
    "In this project, **we set up the vocabulary dictionary by removing the non-frequent words to prevent a model from overfitting.** \n",
    "This is implemeted in [*vocab.ipynb*](https://github.com/ahrimhan/data-science-project/tree/master/project2/src/vocab.ipynb).\n",
    "\n",
    "* After removing all words that have a length <= 1 character, we first construct the vocabulary dictionary based on only reviews in the training dataset (Number of vocabularies: 52,826).\n",
    "* Then, we iterate the vocabulary dictionary again for counting the word occurrences and removing the non-frequent words that have a low occurrence, such as only being used once or none. Thus, remaining vocabularies have the two or more occurrences (Number of filtered vocabularies: 30,819). These filtered vocabularies are saved in [*vocab.txt*](https://github.com/ahrimhan/data-science-project/tree/master/project2/src/vocab.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation_table = str.maketrans('', '', '\\'\"!.,?:;')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# turn a doc into clean tokens\n",
    "vocab = []\n",
    "\n",
    "with open('./vocab/vocab.txt') as f:\n",
    "    vocab = f.read().split() \n",
    "    \n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # remove punctuation from each token\n",
    "    tokens = [w.translate(remove_punctuation_table) for w in tokens]\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # stemming\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(w.lower()) for w in tokens]\n",
    "\n",
    "    # filter out tokens not in vocab\n",
    "    if len(vocab) > 0:\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing\n",
    "\n",
    "Pre-processing mentioned above requires heavy computation. \n",
    "To improve the speed, we parallelized the pre-processing using the *Pool module* in *multiprocessing package*.\n",
    "Since we use a CPU having 8 cores, the size of Pool is set as 8.\n",
    "By using this technique, **we could achieve 6~7 times speed up.** \n",
    "Using the single thread, it takes 10~12 minutes for cleaning up 12500 documents, whereas, using the multiple threads, it takes only 1 minute and 20~40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up for training-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3680155ea048c1833b7470ee0c87ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up for training-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7a1c063b4b4727967268d76e956b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up for test-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17473ad532104cfba3c62bd1aa67650f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up for test-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78c22e3a26b42dcafa2296b9a292699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Serial version of clean_docs function\n",
    "# def clean_docs(documents):\n",
    "#     for doc in tqdm(documents):\n",
    "#         clean_doc(doc)\n",
    "\n",
    "# Parallel version of clean_docs function\n",
    "def clean_docs(documents):\n",
    "    # Since we use a CPU having 8 cores, the size of Pool is set as 8\n",
    "    with Pool(8) as p:\n",
    "        return list(tqdm(p.imap(clean_doc, documents), total=len(documents)))\n",
    "\n",
    "print(\"Cleaning up for training-positive-docs\")\n",
    "cleaned_train_positive_docs = clean_docs(global_train_positive_docs)\n",
    "print(\"Cleaning up for training-negative-docs\")\n",
    "cleaned_train_negative_docs = clean_docs(global_train_negative_docs)\n",
    "print(\"Cleaning up for test-positive-docs\")\n",
    "cleaned_test_positive_docs = clean_docs(global_test_positive_docs)\n",
    "print(\"Cleaning up for test-negative-docs\")\n",
    "cleaned_test_negative_docs = clean_docs(global_test_negative_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nt', 'realli', 'consid', 'conserv', 'nt', 'person', 'offend', 'film', 'pretti', 'clear', 'plot', 'character', 'film', 'secondari', 'messag', 'and', 'messag', 'conserv', 'either', 'evil', 'stupid', 'charact', 'either', 'good', 'american', 'brainless', 'greedi', 'evil', 'conserv', 'there', 'noth', 'clever', 'creativ', 'nt', 'realli', 'mind', 'polit', 'bia', 'nt', 'purpos', 'behind', 'movi', 'and', 'clearli', 'br', 'br', 'on', 'posit', 'side', 'cast', 'wonder', 'chri', 'cooper', 'impress', 'funni', 'first', 'two', 'three', 'time', 'old', 'joke', 'told', 'br', 'br', 'so', 'realli', 'hate', 'conserv', 'probabl', 'enjoy', 'film', 'look', 'someth', 'realist', 'charact', 'stori', 'less', 'better', 'watch', 'someth', 'els']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_test_negative_docs[0].split()[0:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
